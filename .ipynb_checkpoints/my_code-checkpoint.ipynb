{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.01385 1등\n",
    "0.01946 10등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    # iterate through all the columns of a dataframe and modify the data type\n",
    "    #   to reduce memory usage.        \n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureModify(isTrain):\n",
    "    if isTrain:\n",
    "        all_data = pd.read_csv('../train_V2.csv') \n",
    "        all_data = all_data[all_data['maxPlace'] > 1]\n",
    "        all_data = reduce_mem_usage(all_data)\n",
    "        all_data = all_data[all_data['winPlacePerc'].notnull()]\n",
    "    else:\n",
    "        all_data = pd.read_csv('../test_V2.csv')\n",
    "\n",
    "\n",
    "    all_data['matchType'] = all_data['matchType'].map({\n",
    "    'crashfpp':1,\n",
    "    'crashtpp':2,\n",
    "    'duo':3,\n",
    "    'duo-fpp':4,\n",
    "    'flarefpp':5,\n",
    "    'flaretpp':6,\n",
    "    'normal-duo':7,\n",
    "    'normal-duo-fpp':8,\n",
    "    'normal-solo':9,\n",
    "    'normal-solo-fpp':10,\n",
    "    'normal-squad':11,\n",
    "    'normal-squad-fpp':12,\n",
    "    'solo':13,\n",
    "    'solo-fpp':14,\n",
    "    'squad':15,\n",
    "    'squad-fpp':16\n",
    "    })\n",
    "    all_data = reduce_mem_usage(all_data)\n",
    "\n",
    "    print(\"Match size\")\n",
    "    matchSizeData = all_data.groupby(['matchId']).size().reset_index(name='matchSize')\n",
    "    all_data = pd.merge(all_data, matchSizeData, how='left', on=['matchId'])\n",
    "    del matchSizeData\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    all_data.loc[(all_data['rankPoints']==-1), 'rankPoints'] = 0\n",
    "    all_data['_killPoints_rankpoints'] = all_data['rankPoints']+all_data['killPoints']\n",
    "\n",
    "\n",
    "    all_data[\"_Kill_headshot_Ratio\"] = all_data[\"kills\"]/all_data[\"headshotKills\"]\n",
    "    all_data['_killStreak_Kill_ratio'] = all_data['killStreaks']/all_data['kills']\n",
    "    all_data['_totalDistance'] = 0.25*all_data['rideDistance'] + all_data[\"walkDistance\"] + all_data[\"swimDistance\"]\n",
    "    all_data['_killPlace_MaxPlace_Ratio'] = all_data['killPlace'] / all_data['maxPlace']\n",
    "    all_data['_totalDistance_weaponsAcq_Ratio'] = all_data['_totalDistance'] / all_data['weaponsAcquired']\n",
    "    all_data['_walkDistance_heals_Ratio'] = all_data['walkDistance'] / all_data['heals']\n",
    "    all_data['_walkDistance_kills_Ratio'] = all_data['walkDistance'] / all_data['kills']\n",
    "    all_data['_kills_walkDistance_Ratio'] = all_data['kills'] / all_data['walkDistance']\n",
    "    all_data['_totalDistancePerDuration'] =  all_data[\"_totalDistance\"]/all_data[\"matchDuration\"]\n",
    "    all_data['_killPlace_kills_Ratio'] = all_data['killPlace']/all_data['kills']\n",
    "    all_data['_walkDistancePerDuration'] =  all_data[\"walkDistance\"]/all_data[\"matchDuration\"]\n",
    "    all_data['walkDistancePerc'] = all_data.groupby('matchId')['walkDistance'].rank(pct=True).values\n",
    "    all_data['killPerc'] = all_data.groupby('matchId')['kills'].rank(pct=True).values\n",
    "    all_data['killPlacePerc'] = all_data.groupby('matchId')['killPlace'].rank(pct=True).values\n",
    "    all_data['weaponsAcquired'] = all_data.groupby('matchId')['weaponsAcquired'].rank(pct=True).values\n",
    "    all_data['_walkDistance_kills_Ratio2'] = all_data['walkDistancePerc'] / all_data['killPerc']\n",
    "    all_data['_kill_kills_Ratio2'] = all_data['killPerc']/all_data['walkDistancePerc']\n",
    "    all_data['_killPlace_walkDistance_Ratio2'] = all_data['walkDistancePerc']/all_data['killPlacePerc']\n",
    "    all_data['_killPlace_kills_Ratio2'] = all_data['killPlacePerc']/all_data['killPerc']\n",
    "    all_data['_totalDistance'] = all_data.groupby('matchId')['_totalDistance'].rank(pct=True).values\n",
    "    all_data['_walkDistance_kills_Ratio3'] = all_data['walkDistancePerc'] / all_data['kills']\n",
    "    all_data['_walkDistance_kills_Ratio4'] = all_data['kills'] / all_data['walkDistancePerc']\n",
    "    all_data['_walkDistance_kills_Ratio5'] = all_data['killPerc'] / all_data['walkDistance']\n",
    "    all_data['_walkDistance_kills_Ratio6'] = all_data['walkDistance'] / all_data['killPerc']\n",
    "\n",
    "    all_data[all_data == np.Inf] = np.NaN\n",
    "    all_data[all_data == np.NINF] = np.NaN\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    \n",
    "    features = list(all_data.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchSize\")\n",
    "    features.remove(\"matchType\")\n",
    "    if isTrain:\n",
    "        features.remove(\"winPlacePerc\")\n",
    "\n",
    "    \n",
    "    print(\"Mean Data\")\n",
    "    meanData = all_data.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "    meanData = reduce_mem_usage(meanData)\n",
    "    meanData = meanData.replace([np.inf, np.NINF,np.nan], 0)\n",
    "    meanDataRank = meanData.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    meanDataRank = reduce_mem_usage(meanDataRank)\n",
    "    all_data = pd.merge(all_data, meanData.reset_index(), suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\n",
    "    del meanData\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"vehicleDestroys_mean\",\"rideDistance_mean\",\"roadKills_mean\",\"rankPoints_mean\"], axis=1)\n",
    "    all_data = pd.merge(all_data, meanDataRank, suffixes=[\"\", \"_meanRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del meanDataRank\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"numGroups_meanRank\",\"rankPoints_meanRank\"], axis=1)\n",
    "    \n",
    "    all_data = all_data.join(reduce_mem_usage(all_data.groupby('matchId')[features].rank(ascending=False).add_suffix('_rankPlace').astype(int)))\n",
    "\n",
    "    \n",
    "    print(\"Std Data\")\n",
    "    stdData = all_data.groupby(['matchId','groupId'])[features].agg('std').replace([np.inf, np.NINF,np.nan], 0)\n",
    "    stdDataRank = reduce_mem_usage(stdData.groupby('matchId')[features].rank(pct=True)).reset_index()\n",
    "    del stdData\n",
    "    gc.collect()\n",
    "    all_data = pd.merge(all_data, stdDataRank, suffixes=[\"\", \"_stdRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del stdDataRank\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Max Data\")\n",
    "    maxData = all_data.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    maxData = reduce_mem_usage(maxData)\n",
    "    maxDataRank = maxData.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    maxDataRank = reduce_mem_usage(maxDataRank)\n",
    "    all_data = pd.merge(all_data, maxData.reset_index(), suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\n",
    "    del maxData\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"assists_max\",\"killPoints_max\",\"headshotKills_max\",\"numGroups_max\",\"revives_max\",\"teamKills_max\",\"roadKills_max\",\"vehicleDestroys_max\"], axis=1)\n",
    "    all_data = pd.merge(all_data, maxDataRank, suffixes=[\"\", \"_maxRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del maxDataRank\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"roadKills_maxRank\",\"matchDuration_maxRank\",\"maxPlace_maxRank\",\"numGroups_maxRank\"], axis=1)\n",
    "\n",
    "\n",
    "    print(\"Min Data\")\n",
    "    minData = all_data.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    minData = reduce_mem_usage(minData)\n",
    "    minDataRank = minData.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    minDataRank = reduce_mem_usage(minDataRank)\n",
    "    all_data = pd.merge(all_data, minData.reset_index(), suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\n",
    "    del minData\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"heals_min\",\"killStreaks_min\",\"killPoints_min\",\"maxPlace_min\",\"revives_min\",\"headshotKills_min\",\"weaponsAcquired_min\",\"_walkDistance_kills_Ratio_min\",\"rankPoints_min\",\"matchDuration_min\",\"teamKills_min\",\"numGroups_min\",\"assists_min\",\"roadKills_min\",\"vehicleDestroys_min\"], axis=1)\n",
    "    all_data = pd.merge(all_data, minDataRank, suffixes=[\"\", \"_minRank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del minDataRank\n",
    "    gc.collect()\n",
    "    all_data = all_data.drop([\"killPoints_minRank\",\"matchDuration_minRank\",\"maxPlace_minRank\",\"numGroups_minRank\"], axis=1)\n",
    "\n",
    "    \n",
    "    print(\"group Size\")\n",
    "    groupSize = all_data.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "    groupSize = reduce_mem_usage(groupSize)\n",
    "    all_data = pd.merge(all_data, groupSize, how='left', on=['matchId', 'groupId'])\n",
    "    del groupSize\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    print(\"Match Mean\")\n",
    "    matchMeanFeatures = features\n",
    "    matchMeanFeatures = [ v for v in matchMeanFeatures if v not in [\"killPlacePerc\",\"matchDuration\",\"maxPlace\",\"numGroups\"] ]\n",
    "    matchMeanData= reduce_mem_usage(all_data.groupby(['matchId'])[matchMeanFeatures].transform('mean')).replace([np.inf, np.NINF,np.nan], 0)\n",
    "    all_data = pd.concat([all_data,matchMeanData.add_suffix('_matchMean')],axis=1)\n",
    "    del matchMeanData,matchMeanFeatures\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"matchMax\")\n",
    "    matchMaxFeatures = [\"walkDistance\",\"kills\",\"_walkDistance_kills_Ratio\",\"_kill_kills_Ratio2\"]\n",
    "    all_data = pd.merge(all_data, reduce_mem_usage(all_data.groupby(['matchId'])[matchMaxFeatures].agg('max')).reset_index(), suffixes=[\"\", \"_matchMax\"], how='left', on=['matchId'])\n",
    "\n",
    "    print(\"match STD\")\n",
    "    matchMaxFeatures = [\"kills\",\"_walkDistance_kills_Ratio2\",\"_walkDistance_kills_Ratio\",\"killPerc\",\"_kills_walkDistance_Ratio\"]\n",
    "    all_data = pd.merge(all_data, reduce_mem_usage(all_data.groupby(['matchId'])[matchMaxFeatures].agg('std')).reset_index().replace([np.inf, np.NINF,np.nan], 0), suffixes=[\"\", \"_matchSTD\"], how='left', on=['matchId'])\n",
    "\n",
    "\n",
    "    all_data = all_data.drop([\"Id\",\"groupId\"], axis=1)\n",
    "    all_data = all_data.drop([\"DBNOs\",\"assists\",\"headshotKills\",\"heals\",\"killPoints\",\"_killStreak_Kill_ratio\",\"killStreaks\",\"longestKill\",\"revives\",\"roadKills\",\"teamKills\",\"vehicleDestroys\",\"_walkDistance_kills_Ratio\",\"weaponsAcquired\"], axis=1)\n",
    "    all_data = all_data.drop([\"_walkDistance_heals_Ratio\",\"_totalDistancePerDuration\",\"_killPlace_kills_Ratio\",\"_totalDistance_weaponsAcq_Ratio\",\"_killPlace_MaxPlace_Ratio\",\"_walkDistancePerDuration\",\"rankPoints\",\"rideDistance\",\"boosts\",\"winPoints\",\"swimDistance\",\"_kills_walkDistance_Ratio\"], axis=1)\n",
    "    all_data = all_data.drop([\"_Kill_headshot_Ratio\",\"maxPlace\",\"_totalDistance\",\"numGroups\",\"walkDistance\",\"killPlace\"], axis=1)\n",
    "    all_data = reduce_mem_usage(all_data)\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"done\")\n",
    "    features_label = all_data.columns\n",
    "    features_label = features_label.drop('matchId')\n",
    "    if isTrain:\n",
    "        features_label = features_label.drop('winPlacePerc')\n",
    "\n",
    "    gc.collect()\n",
    "    return all_data,features_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1017.83 MB\n",
      "Memory usage after optimization is: 322.31 MB\n",
      "Decreased by 68.3%\n",
      "Memory usage of dataframe is 322.31 MB\n",
      "Memory usage after optimization is: 292.63 MB\n",
      "Decreased by 9.2%\n",
      "Match size\n",
      "Mean Data\n",
      "Memory usage of dataframe is 642.07 MB\n",
      "Memory usage after optimization is: 216.85 MB\n",
      "Decreased by 66.2%\n",
      "Memory usage of dataframe is 757.68 MB\n",
      "Memory usage after optimization is: 212.61 MB\n",
      "Decreased by 71.9%\n",
      "Memory usage of dataframe is 831.23 MB\n",
      "Memory usage after optimization is: 233.25 MB\n",
      "Decreased by 71.9%\n",
      "Std Data\n",
      "Memory usage of dataframe is 758.04 MB\n",
      "Memory usage after optimization is: 212.98 MB\n",
      "Decreased by 71.9%\n",
      "Max Data\n",
      "Memory usage of dataframe is 394.67 MB\n",
      "Memory usage after optimization is: 189.79 MB\n",
      "Decreased by 51.9%\n",
      "Memory usage of dataframe is 757.68 MB\n",
      "Memory usage after optimization is: 212.61 MB\n",
      "Decreased by 71.9%\n",
      "Min Data\n",
      "Memory usage of dataframe is 394.67 MB\n",
      "Memory usage after optimization is: 189.79 MB\n",
      "Decreased by 51.9%\n",
      "Memory usage of dataframe is 757.68 MB\n",
      "Memory usage after optimization is: 212.61 MB\n",
      "Decreased by 71.9%\n",
      "group Size\n",
      "Memory usage of dataframe is 46.39 MB\n",
      "Memory usage after optimization is: 32.86 MB\n",
      "Decreased by 29.2%\n",
      "Match Mean\n",
      "Memory usage of dataframe is 1238.36 MB\n",
      "Memory usage after optimization is: 398.65 MB\n",
      "Decreased by 67.8%\n",
      "matchMax\n",
      "Memory usage of dataframe is 0.96 MB\n",
      "Memory usage after optimization is: 0.69 MB\n",
      "Decreased by 28.6%\n",
      "match STD\n",
      "Memory usage of dataframe is 2.20 MB\n",
      "Memory usage after optimization is: 0.82 MB\n",
      "Decreased by 62.5%\n",
      "Memory usage of dataframe is 3613.29 MB\n",
      "Memory usage after optimization is: 3312.19 MB\n",
      "Decreased by 8.3%\n",
      "done\n",
      "Split time\n"
     ]
    }
   ],
   "source": [
    "X_train,features_label = featureModify(True) \n",
    "\n",
    "print(\"Split time\")\n",
    "def split_train_val(data, fraction):\n",
    "    matchIds = data['matchId'].unique().reshape([-1])\n",
    "    train_size = int(len(matchIds)*fraction)\n",
    "    \n",
    "    random_idx = np.random.RandomState(seed=2).permutation(len(matchIds))\n",
    "    train_matchIds = matchIds[random_idx[:train_size]]\n",
    "    val_matchIds = matchIds[random_idx[train_size:]]\n",
    "    \n",
    "    data_train = data.loc[data['matchId'].isin(train_matchIds)]\n",
    "    data_val = data.loc[data['matchId'].isin(val_matchIds)]\n",
    "    return data_train, data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y time\n",
      "X_train time\n",
      "X test train time\n",
      "X test train winPlace remove\n",
      "X test np time\n",
      "y test np time\n"
     ]
    }
   ],
   "source": [
    "# Split the Data by matchId.\n",
    "X_train, X_train_test = split_train_val(X_train, 0.91)\n",
    "print(\"Y time\")\n",
    "y = X_train['winPlacePerc']\n",
    "y_test = X_train_test['winPlacePerc']\n",
    "print(\"X_train time\")\n",
    "X_train = X_train.drop(columns=['matchId', 'winPlacePerc'])\n",
    "print(\"X test train time\")\n",
    "X_train_test = X_train_test.drop(columns='matchId')\n",
    "print(\"X test train winPlace remove\")\n",
    "X_train_test = X_train_test.drop(columns='winPlacePerc')\n",
    "\n",
    "print(\"X test np time\")\n",
    "X_train_test = np.array(X_train_test)\n",
    "print(\"y test np time\")\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train np time\n",
      "y np time\n",
      "X_train2 np time\n",
      "y2 np time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the Data again and then join it. I am doing this because If I turn the Pandas DataFrame into Numpy Array with \n",
    "# all rows at once, Kernel will be killed for exceeding 16GB Memory. \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_train2, y, y2 = train_test_split(X_train, y, test_size=0.1, shuffle=False)\n",
    "print(\"X_train np time\")\n",
    "X_train = np.array(X_train)\n",
    "print(\"y np time\")\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X_train2 np time\")\n",
    "X_train2 = np.array(X_train2)\n",
    "print(\"y2 np time\")\n",
    "y2 = np.array(y2)\n",
    "\n",
    "y = np.concatenate((y, y2), axis=0)\n",
    "del y2\n",
    "gc.collect()\n",
    "X_train = np.concatenate((X_train, X_train2), axis=0)\n",
    "del X_train2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_reg = LinearRegression()\n",
    "lr_reg.fit(X_train, y)\n",
    "\n",
    "ridge_reg = Ridge()\n",
    "ridge_reg.fit(X_train, y)\n",
    "\n",
    "lasso_reg = Lasso()\n",
    "lasso_reg.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(model):\n",
    "    pred = model.predict(X_train_test)\n",
    "    mse = mean_squared_error(y_test , pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print('{0} 로그 변환된 RMSE: {1}'.format(model.__class__.__name__,np.round(rmse, 3)))\n",
    "    return rmse\n",
    "\n",
    "def get_rmses(models):\n",
    "    rmses = [ ]\n",
    "    for model in models:\n",
    "        rmse = get_rmse(model)\n",
    "        rmses.append(rmse)\n",
    "    return rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression 로그 변환된 RMSE: 0.05000000074505806\n",
      "Ridge 로그 변환된 RMSE: 0.04800000041723251\n",
      "Lasso 로그 변환된 RMSE: 0.12999999523162842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04968881, 0.048239052, 0.12987678]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [lr_reg, ridge_reg, lasso_reg]\n",
    "get_rmses(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
